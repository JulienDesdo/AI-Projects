# üöó Cars_DQLN

Dans mon exploratio de l'intelligence artificielle, j'ai toujours souhait√© me pencher sur une technique combinant les r√©seaux de neurones (**deep-learning**) et le Q-learning (**Reinforcement learning**) car le m√©lange des deux, donnant place aux **Deep Reinforcement Learning** *(que j'abr√®ge en DQLN pour Deep-Q-Learning-Networks)*. 

Les **DQN** ont explos√© en 2015 DeepMind a battu des records sur les jeux Atari en surpassant les humains, sans connaissance particuli√®re du jeu [LIEN DE LA VIDEO DU RECORD]. 


J'ai donc d√©cid√© ‚Äì pour mieux comprendre ces algorithmes ‚Äì de d√©velopper un jeu de voiture o√π l'IA doit atteindre une ligne d'arriv√©e. Ce README explique les bases th√©oriques, le "pourquoi du comment", puis passe en revue les principales briques du projet.

[GIF D ILLUSTRATION] 

---

## Bases Th√©roriques 

Tout d'abord, rappelons qu'il existe diff√©rentes cat√©gories d'algorithmes en intelligence artificielle. On distingue principalement **les mod√®les supervis√©s, non supervis√©s,** ainsi que certains qui se situent √† la fronti√®re entre les deux (parfois appel√©s **semi-supervis√©s** ou **par renforcement**).

- **Un mod√®le supervis√©** apprend √† partir de donn√©es annot√©es. Par exemple, un dataset d‚Äôimages avec des labels de classes (chien, chat, etc.). L'id√©e est de choisir un mod√®le f dont on va affiner les param√®tres gr√¢ce √† des algorithmes d'optimisation. J'en pr√©sente certains dans cette partie du m√™me repo : https://github.com/JulienDesdo/AI-Projects/tree/main/theory/Levenberg-Marquardt. On peut citer le deep learning comme exemple embl√©matique : il ajuste les poids des r√©seaux de neurones progressivement au fil des it√©rations (via des algos comme la descente de gradient). J'explique le fonctionnement standard du deep-learning dans ce rapport :  https://github.com/JulienDesdo/AI-Projects/tree/main/theory/ADAM-MLP-Implementation/Rapport ADAM MLP-5-11.pdf. 

- **Un mod√®le non supervis√©** n'apprend pas √† partir de labels connus. Il se base uniquement sur la structure des donn√©es pour identifier des motifs ou des regroupements. Ce type d'apprentissage est souvent utilis√© pour des t√¢ches comme le clustering (voir l'optimisation mim√©tique, dans la partie 4 de mon rapport Monte Carlo :  https://github.com/JulienDesdo/AI-Projects/tree/main/theory/Mont-Carlo-Algorithms/Rapport-2-14.pdf) ou la r√©duction de dimensionnalit√© (PCA, largement utilis√©e en traitement d'images et de signaux).

- "Entre les deux", on trouve le **reinforcement learning**. Il utilise une structure bien d√©finie (Agent, Environnement, Reward), qui ressemble √† un mod√®le, mais ce n‚Äôest pas aussi explicite qu‚Äôune fonction math√©matique claire reliant des entr√©es √† des sorties comme en apprentissage supervis√©. L‚Äôagent apprend par essais/erreurs en interagissant avec l‚Äôenvironnement, et c‚Äôest cette dynamique qui le distingue des autres approches classiques.

C'est pourquoi avant de parler des r√©seaux de neurones renforc√©s, je vais parler de Q-Learning. 

### Q-Learning 
<br>
   
  #### *Vocabulaire cl√©*
  <br>

  [DESSINS SOURIS LABYRINTHE]
  Prenons l'exemple d'un souris dans un labirynthe. La mod√©lisation d'un environnement RL repose sur les precepts suivants : 
  [SCHEMA AGENT, ENV, ACTION]

  Travailler en Q-Learning c'est d'abord identifier qui fait quoi dans notre cas concret par rapport aux √©l√©ments th√©oriques n√©cessaires. Ici, **l'environnement (E)** est le labirynthe car il constitue l'ensemble de toutes les case possibles. Il poss√®de des **rewards (R)** (fromage) et des **malus** (√©clair). L'agent est la souris qui se d√©place dans le labyrinthe et a quatre options possibles (**actions (A)**) qu'elle peut effectuer sur l'environnement (si celui ci le permet) : aller en bas, haut, gauche, droite. Ce sch√©ma est la base de la th√©orie du RL. L'id√©e est que l'agent va explorer son environnement et chercher √† cumuler une maximum de points (reward) en evitant les malus. Le but ultime serait de chercher le "largets accumulted reward over a sequence of actions". Et du coup on peut voir le reward comme une indication √† l'agent s'il est en train de r√©ussir sa mission ou non ("Indication of agent performance"). 
  Pour faire √ßa on fournit des donn√©es (**observations**) √† la souris pour mettons qu'elle sache ce qu'il y a sur les cases adjacentes. <br> 
  
 * **Note** : L'observation peut √™tre juste les cases adjacentes ou un champ de vision plus large, √ßa depend du design du jeu. Dans un labirynthe simple, c'est souvent les cases adjacentes. Dans les jeux plus complexes (ex: vision 3D), √ßa peut √™tre plus large.* <br>
  
  L'environnement peut √™tre assimil√© √† une matrice o√π chaque case du labirynthe est un element de la matrice. Et un element de cette matrice, c'est √† dire une case sur laquelle se trouve la souris, et un **√©tat** ou **state (S)**.
***********************VERIFIER SI ENV = SET OF STATES + MATRIX OF TRANSITION ou ENV = SET OF STATES + REWARDS... BREF v√©rifier les def precise et affiner. QUOI . !!!! ********************
   <br>
  #### *Chaines de Markov* 
  <br>

**1¬∞ Markov Process (MP)** <br>

Pour r√©ussir √† ce que la souris prenne un d√©cision sur la case sur laquelle elle doit aller √† partir de celle o√π elle se trouve, on peut attribuer des probabilit√©s selon qu'il est bon ou mauvais de s'y rendre. Notre systeme est en fait une **machine √† √©tat** dans laquel on passe d'un √©tat √† un autre gr√¢ce √† une certaine probabilit√© : ce sont les MARKOV PROCESS. Voici un exemple g√©n√©ral :  
***********************VERIFIER MARKOV PROCESS j'ai l'impression de donenr une d√©finition impropre. ******************************************

[SCHEMA MARKOV AVEC PROBAS]

Chaque cercle est un √©tat, pour la souris ce serait un position/case dans le labirynthe. 

Naturellement, puisque le passage d'un √©tat √† un autre se fait en probabilit√©s, on voit que l'on peut regrouper ces probabilit√©s dans une matrice qui d√©crit totalement le syst√®me pr√©c√©dent. 

[SCHEMA MATRICE CORRESPONDANTE]
On appelle de cela la **matrice de probabilit√© (P)** du syst√®me. On pourrait donc dire que le systeme c'est : **l'ensemble des √©tats (S) & la matrice de transistion d'√©tat (P)**

Precisions de vocabulaire : **une chaine de markov** (ensemble d'√©tats li√©s entre eux : state1 -> state2 -> ... -> stateN) ; **History** : S√©quence d'obersation √† travers le temps (exemple : [Chat, Ordinateur, Maison, Chat, Chat, Ordinateur...]) ; **Episode** :  extraire l'observation de l'√©tat de transition ; 

**2¬∞ Markov Reward Process (MRP)** <br>

Seulement, le probleme principal de cette m√©thode est qu'il n'y aucune pr√©voyance. En effet, si la souris a le choix entre un √©clair en haut et un fromage en bas. Mais que dans le premier cas, l'√©clair est suivi de 100 fromages et dans l'autre suivi de 100 √©clair. La souris choisira un fromage et 100 eclair. Ce qui donne une souris grill√©e.<br>
C'est pouquoi on introduit les **MARKOV REWARD PROCESS**. On introduit une variable al√©atoire qui symabolise le gain de l'agent sur une episode. Cela s'√©crit : 

[FORMULE Gt avec somme] 

**Œ≥ est le facteur de pr√©voyance de l'agent entre 0 et 1**. Cela a pour but d'√©valuer la valeur d'un √©tat que l'on choisit en tenant compte de recompances √† plus ou moins long terme. Evaluer les recompanses √† long terme sur un episode c'est approcher Œ≥ de 1, sinon rapprocher Œ≥ de 0 c'est choisir le gain maximum imm√©diat au risque d'y perdre plus (c'est le cas pr√©c√©dent). La value of state V(s) s'exprime : 

[Equation V(S) = E[G | St=s]]

On peut r√©crier cette formule un peu abstraire en une qui s'applique directement au cas du syst√®me √† √©tat : <br>
[V(s)=R(s)+Œ≥s‚Ä≤‚àë‚ÄãP(s‚Ä≤‚à£s)V(s‚Ä≤)] 

Il s'agit de l'**√©quation de Bellman simplifi√©e pour le cas MRP (markov reward process)**. En analysant la formule on voit bien que tout notre proc√©d√© revient √† obtenir le systeme suivant : 
[schema MARKOV avec probas et reward]
[schema matrix of P ; matrix of R]

**Notre systeme MRP se formule (S,P,R,Œ≥)** <br>

Voici un **exemple** pour √™tre un peu didactique : <br>
√âtats : S={A,B}, R(A)=5, R(B)=10, P(A‚à£A)=0.7, P(B‚à£A)=0.3, P(A‚à£B)=0.4, P(B‚à£B)=0.6. En appliquant Bellamn MRP, on a : 
V(A)=5+0.9√ó[0.7V(A)+0.3V(B)] <br>
V(B)=10+0.9√ó[0.4V(A)+0.6V(B)] <br>

**3¬∞ Markov Decision Process (MDP)** <br>



On peut d'instinct remarquer un dernier probleme ? Comment explorer correctement son environnement, dois je en tant que souris, explorer ou exploiter le peu que j'ai explor√© ? C'est tout le dilemne de l'agent qui doit √† la fois poss√©der des donn√©es pour pouvoir pr√©dire ensuite mais en m√™me temps,
   La solution consiste en fait √† predire la solution en fonction de calcul de Q-value, et de changer la mani√®re dont on choisit ses valeurs √† mesure que l'on explore son environnemnt : la selection d'un Q-value s'appelle une politique. Trouver les politique optimale c'est obtenir pi*. On est l√† en plein de un sch√©ma decisionnel : les markov decisions process. Notre systeme est une matrice √† trois dimensions : la matrice d'√©tat auquel on adjoint nos actions. 

(bon des choses √† repr√©cciser!).   
-> probleme d'evolution des actions resolus par decisions process, l√† o√π les actions sont fig√©s dans le cas du reward process. 
<br>

  #### Q-Tables, Bellman Equation, Politiques, Algorithme
<br> <br>


  #### Limites du Q-Learning 
  <br> 

  Dans le cas de notre jeu de voiture, 
  Environnement trop grand
  G√©n√©ralisation impossible
  Impossibilit√© de traiter des entr√©es complexes



### Deep-Q-Networks 

Enumeration des etapes, lien vers video. 1,...10. Policy Netowrk (train), Target Network, Q-Values... 

---

## Programme

Concretement : Libs de reinforcement, stable baseline pour les r√©seaux de neurones. 
Diagramme de classes ? 
Fonctionnalit√©s. 
Explication code + G√©n√©rer eventuellement docs (doxygen?) du script. 

