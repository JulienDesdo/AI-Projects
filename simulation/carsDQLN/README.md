# üöó Cars_DQLN

Dans mon exploratio de l'intelligence artificielle, j'ai toujours souhait√© me pencher sur une technique combinant les r√©seaux de neurones (**deep-learning**) et le Q-learning (**Reinforcement learning**) car le m√©lange des deux, donnant place aux **Deep Reinforcement Learning** *(que j'abr√®ge en DQLN pour Deep-Q-Learning-Networks)*. 

Les **DQN** ont explos√© en 2015 DeepMind a battu des records sur les jeux Atari en surpassant les humains, sans connaissance particuli√®re du jeu [LIEN DE LA VIDEO DU RECORD](https://www.youtube.com/watch?v=z48JCQZwwzA). 


J'ai donc d√©cid√© ‚Äì pour mieux comprendre ces algorithmes ‚Äì de d√©velopper un jeu de voiture o√π l'IA doit atteindre une ligne d'arriv√©e. Ce README explique les bases th√©oriques, le "pourquoi du comment", puis passe en revu les principales briques du projet.

![GIF ANIMATION](images-doc/Animation.gif)

---

## üß† Bases Th√©roriques 

Tout d'abord, rappelons qu'il existe diff√©rentes cat√©gories d'algorithmes en intelligence artificielle. On distingue principalement **les mod√®les supervis√©s, non supervis√©s,** ainsi que certains qui se situent √† la fronti√®re entre les deux (parfois appel√©s **semi-supervis√©s** ou **par renforcement**).

- **Un mod√®le supervis√©** apprend √† partir de donn√©es annot√©es. Par exemple, un dataset d‚Äôimages avec des labels de classes (chien, chat, etc.). L'id√©e est de choisir un mod√®le f dont on va affiner les param√®tres gr√¢ce √† des algorithmes d'optimisation. J'en pr√©sente certains dans cette partie du m√™me repo : https://github.com/JulienDesdo/AI-Projects/tree/main/theory/Levenberg-Marquardt. On peut citer le deep learning comme exemple embl√©matique : il ajuste les poids des r√©seaux de neurones progressivement au fil des it√©rations (via des algos comme la descente de gradient). J'explique le fonctionnement standard du deep-learning dans ce rapport :  https://github.com/JulienDesdo/AI-Projects/blob/main/theory/ADAM-MLP-Implementation/Rapport%20ADAM%20MLP-5-11.pdf. 

- **Un mod√®le non supervis√©** n'apprend pas √† partir de labels connus. Il se base uniquement sur la structure des donn√©es pour identifier des motifs ou des regroupements. Ce type d'apprentissage est souvent utilis√© pour des t√¢ches comme le clustering (voir l'optimisation mim√©tique, dans la partie 4 de mon rapport Monte Carlo :  https://github.com/JulienDesdo/AI-Projects/tree/main/theory/Mont-Carlo-Algorithms/Rapport-2-14.pdf) ou la r√©duction de dimensionnalit√© (PCA, largement utilis√©e en traitement d'images et de signaux).

- "Entre les deux", on trouve le **reinforcement learning**. Il utilise une structure bien d√©finie (Agent, Environnement, Reward), qui ressemble √† un mod√®le, mais ce n‚Äôest pas aussi explicite qu‚Äôune fonction math√©matique claire reliant des entr√©es √† des sorties comme en apprentissage supervis√©. L‚Äôagent apprend par essais/erreurs en interagissant avec l‚Äôenvironnement, et c‚Äôest cette dynamique qui le distingue des autres approches classiques.

C'est pourquoi avant de parler des r√©seaux de neurones renforc√©s, je vais parler de Q-Learning. 

### Q-Learning 

   
  #### *Vocabulaire cl√©*
  <br>

  ![mouse-labirynthe](images-doc/mouse-labirynthe.png)
  <br>
  Prenons l'exemple d'un souris dans un labyrinthe. La mod√©lisation d'un environnement RL repose sur les precepts suivants : 
  
  ![SCHEMA AGENT, ENV, ACTION](images-doc/RL-components.png) <br>

  Travailler en Q-Learning c'est d'abord identifier qui fait quoi dans notre cas concret par rapport aux √©l√©ments th√©oriques n√©cessaires. Ici, **l'environnement (E)** est le labyrinthe car il constitue l'ensemble de toutes les case possibles. Il poss√®de des **rewards (R)** (fromage) et des **malus** (√©clair). L'agent est la souris qui se d√©place dans le labyrinthe et a quatre options possibles (**actions (A)**) qu'elle peut effectuer sur l'environnement (si celui ci le permet) : aller en bas, haut, gauche, droite. Ce sch√©ma est la base de la th√©orie du RL. L'id√©e est que l'agent va explorer son environnement et chercher √† cumuler un maximum de points (reward) en evitant les malus. Le but ultime serait de chercher le "largets accumulted reward over a sequence of actions". Et du coup on peut voir le reward comme une indication √† l'agent s'il est en train de r√©ussir sa mission ou non ("Indication of agent performance"). 
  Pour faire √ßa on fournit des donn√©es (**observations**) √† la souris pour qu'elle sache ce qu'il y a sur les cases adjacentes. <br> 
  
 * **Note** : L'observation peut √™tre juste pour les cases adjacentes ou un champ de vision plus large, √ßa d√©pend du design du jeu. Dans un labyrinthe simple, c'est souvent les cases adjacentes. Dans les jeux plus complexes (ex: vision 3D), √ßa peut √™tre plus large. En RL classique, cela revient √† conna√Ætre tout le labyrinthe (ce qu'il y a sur chaque case). Mais il existe aussi le RL partiellement observable (POMDP) pour une observabilit√© r√©duite.* <br>
  
L'environnement peut √™tre repr√©sent√© sous forme d'une **matrice**, o√π chaque case du labyrinthe correspond √† un √©l√©ment unique. Chaque √©l√©ment de cette matrice ‚Äî c‚Äôest-√†-dire chaque case sur laquelle la souris peut se trouver ‚Äî d√©finit un **√©tat** ou **state (S)**.  

En pratique, l'environnement englobe non seulement l'ensemble des √©tats accessibles, mais aussi les r√®gles du jeu : c'est-√†-dire quelles actions sont possibles √† partir de chaque √©tat, comment l'agent peut √©voluer apr√®s une action, et quelles r√©compenses il re√ßoit en fonction des choix effectu√©s. Cette structure globale permet de mod√©liser de mani√®re compl√®te l'interaction entre l'agent et son monde virtuel, m√™me si, pour l'instant, on se concentre simplement sur la repr√©sentation des √©tats via la matrice du labyrinthe.
   <br>
  #### *Chaines de Markov* 
  <br>

**1¬∞ Markov Process (MP)** <br>

Pour que la souris d√©cide vers quelle case aller √† partir de sa position actuelle, on peut mod√©liser la dynamique de ses d√©placements sous forme de probabilit√©s : par exemple, quelle est la probabilit√© qu‚Äôelle atteigne telle ou telle case apr√®s une action donn√©e ? Notre syst√®me devient alors **une machine √† √©tats** o√π l‚Äôon passe d‚Äôun √©tat √† un autre selon certaines r√®gles probabilistes : c‚Äôest ce qu‚Äôon appelle un **processus de Markov**. Voici un sch√©ma : 

![SCHEMA Markov MP](images-doc/state-machine-markov-process.png)

Chaque cercle est un √©tat, pour la souris ce serait un position/case dans le labyrinthe. 

Naturellement, puisque le passage d'un √©tat √† un autre se fait en probabilit√©s, on voit que l'on peut regrouper ces probabilit√©s dans une matrice qui d√©crit totalement le syst√®me pr√©c√©dent. 

![Matrice P](images-doc/P-matrix.png)
<br>
On appelle de cela la **matrice de transition (P)** du syst√®me. On pourrait donc dire que le systeme c'est : **l'ensemble des √©tats (S) & la matrice de transition d'√©tat (P)**
<br>
Precisions de vocabulaire : 
- **Chaine de markov** : ensemble d'√©tats li√©s entre eux : state1 -> state2 -> ... -> stateN) ; 
- **History** : S√©quence d'observation √† travers le temps (exemple : [A, B, C, A, A, B...]) ; 
- **Episode** :  extraire l'observation de l'√©tat de transition ;  

**2¬∞ Markov Reward Process (MRP)** <br>

Seulement, le probl√®me principal de cette m√©thode est qu'il n'y a aucune pr√©voyance. En effet, si la souris a le choix entre un √©clair en haut et un fromage en bas. Mais que dans le premier cas, l'√©clair est suivi de 100 fromages et dans l'autre suivi de 100 √©clairs. La souris choisira un fromage et 100 eclairs. Ce qui donne une souris grill√©e.<br>
C'est pouquoi on introduit les **MARKOV REWARD PROCESS**. On introduit une variable al√©atoire qui symbolise le gain de l'agent sur une episode. Cela s'√©crit : 

![FORMULE Gt avec somme](images-doc/expression-gain.png)

**Œ≥ est le facteur de pr√©voyance de l'agent entre 0 et 1**. Cela a pour but d'√©valuer la valeur d'un √©tat que l'on choisit en tenant compte de r√©compenses √† plus ou moins long terme. Evaluer les recompanses √† long terme sur un episode c'est approcher Œ≥ de 1, sinon rapprocher Œ≥ de 0 c'est choisir le gain maximum imm√©diat au risque d'y perdre plus (c'est le cas pr√©c√©dent). La value of state V(s) s'exprime : 

![Equation V(S) = E[G | St=s]](images-doc/value-of-state.png) 

On peut r√©√©crire cette formule un peu abstraite en une qui s'applique directement au cas du syst√®me √† √©tat : <br>
![V(s)=R(s)+Œ≥s‚Ä≤‚àë‚ÄãP(s‚Ä≤‚à£s)V(s‚Ä≤)](images-doc/Bellman-simplifiee.png)

Il s'agit de l'**√©quation de Bellman simplifi√©e pour le cas MRP (markov reward process)**. En analysant la formule on voit bien que tout notre proc√©d√© revient √† obtenir le systeme suivant : 
<br>
![schema MARKOV avec probas et reward](images-doc/state-machine-markov-reward-process.png)
<br>
![schema matrix of P ; matrix of R](images-doc/matrix.png)

**Notre systeme MRP se formule (S,P,R,Œ≥)** <br><br>

Voici un **exemple** pour √™tre un peu didactique : <br>
√âtats : S={A,B}, R(A)=5, R(B)=10, P(A‚à£A)=0.7, P(B‚à£A)=0.3, P(A‚à£B)=0.4, P(B‚à£B)=0.6. 
<br>
En appliquant l'√©quation de Bellman pour MRP, on a : 

V(A)=5+0.9√ó[0.7V(A)+0.3V(B)] <br>
V(B)=10+0.9√ó[0.4V(A)+0.6V(B)] <br>

**3¬∞ Markov Decision Process (MDP)** <br>

**Syst√®me Complet**

Un dernier point important appara√Æt naturellement : comment l'agent doit-il agir ? Dois-je, en tant que souris, explorer de nouvelles zones du labyrinthe (au risque de tomber sur un pi√®ge), ou bien exploiter ce que je connais d√©j√† pour maximiser mon gain imm√©diat ?

 Ce dilemme entre exploration et exploitation est central en apprentissage par renforcement.

Pour mod√©liser √ßa, on utilise ce qu'on appelle un Markov Decision Process (MDP). C‚Äôest la suite logique du MRP, sauf qu‚Äôici on prend en compte les actions de l'agent. Dans un MDP : <br>

- √Ä chaque **√©tat s**, l'agent peut choisir une **action a** parmi un ensemble d‚Äôactions possibles.
- Chaque **couple (s,a)** conduit √† une transition vers un nouvel √©tat s‚Ä≤ avec une probabilit√© P(s‚Ä≤‚à£s,a), et offre une r√©compense R(s,a).

R√©sultat : le syst√®me devient une **matrice √† trois dimensions** : Pour un √©tat s on poss√®de plusieurs actions r√©alisable. Dans le cas de la souris, on a une **matrice tri-dimensionnelle de taille 4x4x4 (longueur x largeur x actions)**. 

![sch√©ma matrice en 3D](images-doc/state-machine-markov-decision-process.png)

La question est maintenant de savoir ce que contient cette matrice ? Elle contient en r√©alit√© les Q-values, qui aide √† la prise de d√©cision √† l'aide de la politique. 

**Equation de Bellman**

Mais au fait, ces fameuses Q-values, c‚Äôest quoi ?

Eh bien c‚Äôest l√† qu‚Äôarrive l'√©quation de Bellman, qui fait le lien entre tout ce qu‚Äôon a vu avant : √©tat, action, r√©compense, probabilit√© de transition. L‚Äôid√©e : calculer pour chaque couple (√©tat, action) la ‚Äúvaleur‚Äù de cette action (ce qu‚Äôon peut esp√©rer gagner en la prenant).

![equation de bellman compllete avec politique pi, value of state, reward et la proba](images-doc/Bellman-equation.png)

- **Œ≥** est le facteur de pr√©voyance (d√©j√† vu avant).
- La somme parcourt tous les √©tats accessibles apr√®s **(s,a)**, avec la meilleure action suivante.
- **œÄ** est la politique (expliqu√© dans la suite). 

Concr√®tement, dans notre exemple de la souris, √† chaque case s, elle peut aller en haut, bas, gauche, droite. On a donc pour un √©tat s donn√©e : **Q(s,haut), Q(s,bas), Q(s,droite), Q(s,gauche)**. 

<br>

**Politique**

La politique consiste donc √† s√©lectionner une valeur Q-value √† un √©tat s, donc choisir une action √† effectuer pour stocker la Q-value associ√©e. Simplement **Politique : Strat√©gie pour choisir les actions a**. Mais qu'est ce qu'une bonne politique ? Comment la choisir ? <br>

Tout d√©pend de ce que l'on veut faire, tout d'abord rappelons que m√™me si Œ≥ est surtout le facteur de pr√©voyance, il a aussi un impact indirect sur l'exploration. En effet : 
 - Œ≥ proche de 0 ‚Üí l‚Äôagent ne cherche que le gain imm√©diat (exploration tr√®s locale).
 - Œ≥ proche de 1 ‚Üí il regarde loin dans le futur et peut apprendre des strat√©gies globales m√™me s'il n'a pas explor√© tout √† fond. Pour faire simple, les √©tats peu visit√©s peuvent √™tre ‚Äúappris par approximation‚Äù gr√¢ce √† la propagation des valeurs via Œ≥. <br>

En th√©orie, pour apprendre correctement, chaque couple (√©tat, action) doit √™tre explor√© suffisamment. Mais en pratique, **quand Œ≥ est √©lev√©, l‚Äôagent peut ‚Äúdeviner‚Äù ou approcher la valeur de certains √©tats peu visit√©s, car les r√©compenses futures se propagent dans le calcul via l'√©quation de Bellman.**

La politique est √† la fois la s√©lection des Q-value pour un bon apprentissage. Mais tout d√©pend de ce que l'on veut faire. A la mani√®re de Œ≥ qui permet soit une exploration gloal ou locale, la politique permet de faire la m√™me chose. <br>

On a : <br>
- **Greedy** : choisir toujours l‚Äôaction qui a la meilleure Q-value. A utiliser une fois que l'on a suffisement appris. œÄ(s) = argmax[a] (Q(s,a))
- **Epsilon-Greedy** : on choisit un action al√©atoire avec un param√®tre epsilon (exploration) ou on minimise Q(s,a) (exploitation)
- **D√©terministe** : toujours la m√™me action pour un √©tat donn√©. œÄ(s)=a
- **Stochastique** : on tire au hasard en fonction d‚Äôune proba. œÄ(a/s) = P(At=a/St=s)

Ainsi selon notre choix de politique on fait soit : <br>
- De **l'exploration** : tester de nouvelles actions pour d√©couvrir des chemins ou des strat√©gies qu‚Äôon ne conna√Æt pas encore.
- De **l'exploitation** : utiliser ce qu‚Äôon a d√©j√† appris pour maximiser le gain √† court terme.

L‚Äôexploration permet d‚Äô√©viter de passer √† c√¥t√© d‚Äôune meilleure solution (m√™me si √ßa peut co√ªter cher sur le moment), alors que l‚Äôexploitation permet de capitaliser tout de suite sur ce qu‚Äôon pense √™tre la meilleure strat√©gie. Trouver le bon √©quilibre entre les deux est essentiel pour apprendre efficacement. **C‚Äôest donc un enjeu majeur pour tout syst√®me RL, y compris dans des m√©thodes avanc√©es comme les DQN ou PPO, o√π l‚Äôon cherche √† apprendre des politiques plus sophistiqu√©es pour mieux g√©rer ce compromis.** <br>

**Programmabilit√©** 
<br>
√Ä partir de l√†, le sch√©ma est clair : <br>
1Ô∏è‚É£ Pour un √©tat s, je calcule Q(s,a) pour toutes les actions. <br>
2Ô∏è‚É£ Je choisis mon action selon la politique. <br>
3Ô∏è‚É£ Je fais l‚Äôaction, j‚Äôobserve la r√©compense + le nouvel √©tat. <br>
4Ô∏è‚É£ Je mets √† jour Q(s,a). <br>
5Ô∏è‚É£ Je recommence. <br>

![sch√©ma algo flow ici](images-doc/boucle-MDP.png)

Pour ce qui est de la Q-table au d√©but on l'initilise soit √† 0 soit avec des valeurs al√©atoires. Qu'en est t il des matrices (R) et (P) ? 
Dans les cas pratiques on ne connait pas forcement les matrices de transitions. Donc : 
- Cas o√π R et P sont connus : on peut appliquer directement Bellman (programmation dynamique, it√©ration de valeur, etc.). C'est l'√©quation de Bellamn. 
- Cas o√π R et P sont inconnus (la norme en pratique) : ‚Üí on apprend en essayant : Q-learning (l'agent d√©couvre tout seul les ‚Äúpoids‚Äù r√©els de R et P).

A noter que j'ai suppos√© que les fonctions P(s‚Ä≤ ‚à£ s,a) et R(s,a) sont connues. C'est le cas "id√©al" o√π l'environnement est parfaitement mod√©lis√©, ce qui permet d'appliquer directement l'√©quation de Bellman. Ces informations sont en fait souvent inconnues : l'agent doit les d√©couvrir par essai/erreur (ex : Q-learning). On diff√©rencie donc : 

| **Type de RL**                 | **Mod√®le connu (mod√®le-based RL)**                                                                 | **Mod√®le inconnu (mod√®le-free RL)**                                               |
|---------------------------------|---------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------|
| **D√©finition**                  | L'agent conna√Æt la fonction de transition \( P(s'/s,a) \) et la r√©compense \( R(s,a) \).         | L'agent ne conna√Æt ni \( P \) ni \( R \).                                         |
| **M√©thode d'apprentissage**     | Utilise directement les √©quations th√©oriques (ex : Bellman).                                      | Apprend en exp√©rimentant (essai/erreur).                                          |
| **Exemples d'algos**            | Programmation dynamique, Value Iteration, Policy Iteration.                                       | Q-learning, SARSA, DQN, PPO...                                                    |
| **Avantages**                   | Solution exacte si le mod√®le est parfaitement connu.                                             | Plus flexible : fonctionne m√™me sans mod√®le pr√©cis.                               |
| **Inconv√©nients**               | N√©cessite de conna√Ætre l'environnement (souvent irr√©aliste).                                      | Peut √™tre long √† apprendre et n√©cessiter beaucoup d'essais.                       |

<br>

  #### Limites du Q-Learning 
  <br> 

Dans le cas de notre jeu de voiture, on se rend vite compte des limites du Q-Learning classique : <br>
- **Environnement trop grand** : Si l‚Äôespace des √©tats est √©norme (par exemple, chaque case du circuit + la vitesse + l‚Äôangle + plein d'autres param√®tres), la Q-table devient ing√©rable : il faut m√©moriser chaque combinaison √©tat-action.
- **Scalabilit√© limit√©e** : Si l‚Äô√©tat est une image (par exemple une vue du circuit en 2D ou 3D), c‚Äôest encore pire : impossible de stocker toutes les combinaisons image/action dans une simple table ‚Üí explosion m√©moire garantie.
- **G√©n√©ralisation impossible** : Le Q-learning ne g√©n√©ralise pas : deux √©tats tr√®s proches (par ex. une voiture en (x=10,y=50,v=80‚Äâkm/h)(x=10,y=50,v=80km/h) et une autre en (x=10.1,y=50,v=80‚Äâkm/h)(x=10.1,y=50,v=80km/h)) sont trait√©s comme des √©tats totalement diff√©rents. ‚ûî R√©sultat : **Q(s1,a)‚â†Q(s2,a) m√™me si s1‚âàs2**. Le Q-learning ne fait aucune diff√©rence : il ne "comprend" pas que ces deux √©tats sont presque identiques.
- **Exploration inefficace** : Dans un environnement tr√®s grand, l'agent va passer son temps √† explorer des zones qui n'ont aucun int√©r√™t. Il risque de passer √† c√¥t√© des bonnes zones ou d'apprendre trop lentement.
<br>

La cons√©quence ? Le Q-learning classique ne passe pas √† l‚Äô√©chelle d√®s que l‚Äôenvironnement devient un peu trop complexe ou riche. <br>

#### Solution : Deep Q-Network (DQN)

L‚Äôid√©e cl√© est de remplacer la Q-table par un r√©seau de neurones profond (DNN) qui approxime Q(s,a)Q(s,a).Au lieu de stocker une valeur par (√©tat, action), le mod√®le apprend une fonction qui pr√©dit la Q-value √† partir de l‚Äô√©tat et de l‚Äôaction. <br>

Avantages : <br>
- **Capacit√© de g√©n√©ralisation** : des √©tats proches donneront des Q-values similaires.
- **Scalabilit√©** : peut g√©rer des entr√©es complexes comme des images (ex : une cam√©ra embarqu√©e sur la voiture).
- **Apprentissage plus efficace** : le r√©seau partage l'information entre √©tats similaires, ce qui √©vite de devoir tout explorer case par case.

### Deep-Q-Networks 

Comme expliqu√© pr√©c√©demment, dans le DQN, la Q-Table est remplac√©e par un r√©seau de neurones profond. Le r√©seau apprend √† approximer la fonction Q-value :

[loss-formula-Q-network](images-doc/loss-formula-Q-network.png)

Pour rappel : 
- **Œ∏** : les param√®tres (poids) du r√©seau principal (policy network),
- **Œ∏‚àí** : les param√®tres du r√©seau cible (target network),
- **R** : la r√©compense obtenue,
- **Œ≥** : facteur de pr√©voyance,
- **s,a** : √©tat et action courante,
- **s‚Ä≤,a‚Ä≤** : √©tat suivant et toutes les actions possibles.

L'objectif est de r√©duire la diff√©rence entre ce que le r√©seau estime Q(s,a) et ce qu'il ‚Äúdevrait‚Äù valoir selon Bellman (la cible).

Dans l'id√©e : 
[neural-network-Q-values](images-doc/neural-network-Q-values.png)

Mais on rencontre plusieurs probl√®mes.
- **Probleme : Corr√©lation temporelle entre exp√©riences** : Quand on utilise juste un r√©seau, chaque nouvelle exp√©rience influence directement l‚Äôapprentissage ‚ûî cela cr√©e une corr√©lation temporelle forte entre les exp√©riences cons√©cutives, ce qui peut rendre l‚Äôapprentissage instable.
- **Solution : Experience Replay** : On stocke toutes les exp√©riences pass√©es sous forme de tuples (s,a,r,s‚Ä≤)(s,a,r,s‚Ä≤) dans une m√©moire tampon (replay buffer). Lors de l‚Äôapprentissage, on √©chantillonne al√©atoirement des mini-lots √† partir de ce buffer. ‚ûî Cela brise la corr√©lation temporelle entre exp√©riences successives et rend l‚Äôapprentissage plus stable.
- **Probl√®me : Instabilit√© des cibles Q** : Si on met √† jour en permanence le m√™me r√©seau pour pr√©dire ET √©valuer les cibles Q(s‚Ä≤,a‚Ä≤), le mod√®le peut devenir instable ou m√™me diverger.
- **Solution : Target Network** : On introduit un second r√©seau stable, le target network. Ce r√©seau est copi√© p√©riodiquement (tous les N √©pisodes par ex.) √† partir du r√©seau principal. Cela permet de garder des cibles fixes pendant un certain temps, ce qui stabilise l'apprentissage. 

On a donc 2 r√©seaux de neurones : 
- **Policy Network (r√©seau principal)** : celui qui est entra√Æn√© activement pour approximer Q(s,a)
- **Target Network (r√©seau cible)** : une copie fig√©e temporairement du policy network, utilis√©e pour calculer la cible de Bellman.

[schema-DQN](images-doc/schema-DQN.png) 

Etapes du proessus DQN : 
1Ô∏è‚É£ Policy Network (Entra√Ænement) : Le r√©seau principal prend en entr√©e un √©tat (par ex. la position de la voiture sous forme de vecteur ou image) et pr√©dit les Q-values pour toutes les actions possibles. <br>
2Ô∏è‚É£ Target Network (Cible) : Sert √† calculer la cible Bellman R+Œ≥max‚Å°a‚Ä≤Q(s‚Ä≤,a‚Ä≤)R+Œ≥maxa‚Ä≤‚ÄãQ(s‚Ä≤,a‚Ä≤) avec des poids fig√©s. <br>
3Ô∏è‚É£ Action : L'agent choisit une action selon sa politique (par ex. epsilon-greedy) et l‚Äôex√©cute dans l‚Äôenvironnement. <br>
4Ô∏è‚É£ Stockage : On stocke (s,a,r,s‚Ä≤)(s,a,r,s‚Ä≤) dans la Replay Memory. <br>
5Ô∏è‚É£ Apprentissage : √Ä chaque it√©ration, on √©chantillonne un mini-lot d‚Äôexp√©riences du buffer, on passe les √©tats dans le policy network, on calcule les cibles via le target network, et on met √† jour les poids du policy network par r√©tropropagation. <br>
6Ô∏è‚É£ Mise √† jour p√©riodique : Tous les N √©pisodes, on copie les poids du policy network dans le target network. <br>
7Ô∏è‚É£ üîÅ R√©p√©ter le processus jusqu‚Äô√† convergence. <br>

R√©f√©rez-vous √† cette vid√©o pour entrer plus dans les d√©tails :
üé• [Deep Q-Learning/Deep Q-Network (DQN) Explained | Python Pytorch Deep Reinforcement Learning](https://www.youtube.com/watch?v=EUrWGTCGzlA)

---

## üõ†Ô∏èProgramme

Concretement : Libs de reinforcement, stable baseline pour les r√©seaux de neurones. 
Diagramme de classes ? 
Fonctionnalit√©s. 



---

## üìö R√©f√©rences & Liens Utiles 

- **Deep Reinforcement Learning Hands-On**, *Maxim Lapan*, Third Edition, Packt ‚Äî *A practical and easy-to-follow guide to RL, from Q-Learning and DQNs to PPO and RLHF.*
- **Pygame Documentation 1** : [https://he-arc.github.io/livre-python/pygame/index.html](https://he-arc.github.io/livre-python/pygame/index.html)
- **Pygame Documentation 2** : [https://www.pygame.org/news](https://www.pygame.org/news)
- **OpenAI Spinning Up (Reinforcement Learning guide)** : [https://spinningup.openai.com/en/latest/](https://spinningup.openai.com/en/latest/)
- **Stable-Baselines3 (Reinforcement Learning Library)** : [https://github.com/DLR-RM/stable-baselines3](https://github.com/DLR-RM/stable-baselines3)
- **YouTube Videos :**  
  - [Google's DeepMind AI Just Taught Itself To Walk](https://www.youtube.com/watch?v=gn4nRCC9TwQ)  
  - [AI Learns to Speedrun Mario](https://www.youtube.com/watch?v=OQitI066aI0)  
  - [MarI/O - Machine Learning for Video Games](https://www.youtube.com/watch?v=qv6UVOQ0F44&t=185s)
- **Github / Simulations IA (Jean Buffer)** ‚Äî Vid√©os de simulation par IA + code source : [https://github.com/johnBuffer](https://github.com/johnBuffer)
