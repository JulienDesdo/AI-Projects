# ğŸ§  Comparaison dâ€™Optimiseurs : MLP avec Adam vs SGD

Ce projet implÃ©mente un MLP **entiÃ¨rement codÃ© Ã  la main** (forward et backpropagation) pour comparer deux stratÃ©gies dâ€™optimisation : **SGD** (Stochastic Gradient Descent) et **Adam**.
Les optimiseurs modernes comme ADAM, Adagrad, RMSprop ajustent dynamiquement le taux d'apprentissage pour amÃ©liorer la convergence. 

ğŸ¯ **Objectif** :  
Comprendre lâ€™influence de lâ€™optimiseur sur lâ€™apprentissage dâ€™un rÃ©seau de neurones, en particulier la vitesse et la qualitÃ© de convergence.

ğŸ” **Approche** :  
- Mise en Å“uvre dâ€™un MLP minimaliste (une couche cachÃ©e)
- Comparaison entre deux runs :  l'une avec **SGD**, l'autre avec **ADAM**. 
- Visualisation des zones de dÃ©cision pour chaque modÃ¨le

ğŸ“ˆ **Conclusion** :  
Les rÃ©sultats montrent clairement quâ€™**Adam sâ€™adapte mieux** Ã  la topologie du problÃ¨me, confirmant ses avantages dans des contextes Ã  faible tuning ou Ã  gÃ©omÃ©trie complexe.

ğŸ–¼ï¸ Exemple de sortie :  
![RÃ©sultat](./image.png)

ğŸ“„ Pour plus de dÃ©tails sur les fondements thÃ©oriques, le choix des paramÃ¨tres et les rÃ©sultats expÃ©rimentaux :  
[ğŸ“˜ Rapport complet (PDF)](./Rapport%20ADAM%20MLP-5-11.pdf)
