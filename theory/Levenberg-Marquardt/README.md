# ğŸ“ˆ Optimisation NumÃ©rique avec Levenberg-Marquardt

Ce projet met en Å“uvre lâ€™algorithme **Levenberg-Marquardt**, algorithme d'optimisation. Il s'agit peu ou prou d'un algorithme type Gradient, Newton... si ce n'est qu'il plus performant. 
Ce type d'algorithme sert en IA pour ajuster les paramÃ¨tres dâ€™un modÃ¨le afin de minimiser une fonction de coÃ»t, notamment dans lâ€™apprentissage supervisÃ©.

## ğŸ¯ Objectifs

- ImplÃ©mentation de Levenberg-Marquardt avec jacobienne analytique
- Ã‰tude de la sensibilitÃ© aux conditions initiales et au bruit
- Exploration de lâ€™impact du paramÃ¨tre de rÃ©gularisation (`mu`)
- Comparaison avec les mÃ©thodes de descente de gradient

> ğŸ” BasÃ© sur un **TP acadÃ©mique**, reproduit et enrichi dans un cadre personnel. **Les consignes initiales ne sont pas publiÃ©es pour respecter la confidentialitÃ© des contenus pÃ©dagogiques.**

---

## ğŸ§ª ExpÃ©riences

### 1. ModÃ¨le Ã  4 paramÃ¨tres

- ModÃ¨le : `y = x3 * exp(x1 * t) + x4 * exp(x2 * t)`
- Analyse des rÃ©sultats selon diffÃ©rentes conditions initiales et niveaux de bruit
- Ã‰tude des minima locaux et du comportement de la convergence

### 2. ModÃ¨le Ã  10 paramÃ¨tres

- ProblÃ¨me plus complexe avec un espace de recherche plus large
- Ajustement du nombre dâ€™itÃ©rations et contrÃ´le renforcÃ© des paramÃ¨tres

### Bonus : MÃ©thodes du gradient

- Comparaison directe avec la descente de gradient
- Observation d'une moins bonne robustesse face aux minima locaux

---

## ğŸ“ Fichiers

- Scripts Matlab (`Exercice 1`, `Exercice 2`, `dirLM.m`, etc.)
- `Document Explicatif Optimisation.pdf` : rapport dÃ©taillÃ©

---

## ğŸ§  En rÃ©sumÃ©

Le TP montre bien lâ€™intÃ©rÃªt de Levenberg-Marquardt comme mÃ©thode hybride efficace entre gradient et Gauss-Newton, tout en rÃ©vÃ©lant ses limites face aux mauvaises conditions initiales ou Ã  des problÃ¨mes trop bruitÃ©s.
