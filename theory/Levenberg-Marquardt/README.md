# ğŸ“ˆ Optimisation avec Levenberg-Marquardt

Ce projet propose une implÃ©mentation complÃ¨te de lâ€™algorithme **Levenberg-Marquardt**, un compromis entre la descente de gradient et la mÃ©thode de Gauss-Newton, appliquÃ© Ã  des problÃ¨mes de rÃ©gression non linÃ©aire.

## ğŸ¯ Objectifs

- ImplÃ©menter une optimisation robuste sur des fonctions non linÃ©aires
- Comprendre le rÃ´le du paramÃ¨tre de rÃ©gularisation (lambda)
- Observer lâ€™Ã©volution de la convergence et la sensibilitÃ© aux points initiaux
- Visualiser lâ€™impact des diffÃ©rentes mÃ©triques dâ€™erreur

> ğŸ” BasÃ© sur un **TP acadÃ©mique**, reproduit et enrichi dans un cadre personnel. **Les consignes initiales ne sont pas publiÃ©es pour respecter la confidentialitÃ© des contenus pÃ©dagogiques.**

---

## ğŸ§ª Contenu

Le projet est structurÃ© autour de 3 parties principales, chacune testant un cas diffÃ©rent dâ€™optimisation :

### 1. ğŸ“ Ajustement linÃ©aire complexe (EXO 1)
Courbe expÃ©rimentale Ã  modÃ©liser Ã  lâ€™aide de fonctions exponentielles. Ã‰tude des performances selon diffÃ©rents critÃ¨res.

### 2. ğŸ§¬ Ajustement sigmoÃ¯de (EXO 2)
RÃ©gression sur une fonction sigmoÃ¯de. Analyse de la sensibilitÃ© au point initial.

### 3. ğŸ”§ Fonction simple (BONUS)
RÃ©gression sur une fonction de type sinus, avec contrÃ´le des paramÃ¨tres de rÃ©gularisation. Visualisation de la convergence et des rÃ©sidus.

---

## ğŸ“ Fichiers clÃ©s

- `BONUS Gradient`, `Exerice 1` et `Exercice 2` pour le code. 
- `Document Explicatif Optimisation.pdf` : rapport expliquant les Ã©tapes, le raisonnement et les rÃ©sultats
- `TPfinal_app.pdf` : support original non publiÃ© dans le dÃ©pÃ´t public

---

## âœ… RÃ©sultats observÃ©s

- Levenberg-Marquardt offre une **trÃ¨s bonne stabilitÃ©** mÃªme sur des formes complexes
- Le choix du lambda initial et de son ajustement est **dÃ©terminant**
- Les erreurs (MSE, MAE...) peuvent donner des indications complÃ©mentaires selon la forme de la fonction cible

---

## ğŸ§  Ce que jâ€™en retiens

Un excellent TP pour mieux apprÃ©hender le lien entre dÃ©rivÃ©es, ajustement local, et comportement global de lâ€™optimisation. Il permet aussi de mieux sentir les limites des approches purement descendantes ou purement newtoniennes.

---

